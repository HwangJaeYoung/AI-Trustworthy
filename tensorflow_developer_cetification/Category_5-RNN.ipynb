{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Category_5-RNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMwDRd3aAcvnuS0pgfpowpz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"w0tb5B38nV9P","executionInfo":{"status":"ok","timestamp":1643990427717,"user_tz":-540,"elapsed":457,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}}},"outputs":[],"source":["import csv\n","import tensorflow as tf\n","import numpy as np\n","import urllib\n","\n","from tensorflow.keras.layers import Dense, LSTM, Lambda, Conv1D\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.losses import Huber"]},{"cell_type":"code","source":["url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n","urllib.request.urlretrieve(url, 'sunspots.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7SGmGSJwnby5","executionInfo":{"status":"ok","timestamp":1643990428333,"user_tz":-540,"elapsed":8,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}},"outputId":"e7bb6766-41a6-4c9b-bddc-0a0c0125393f"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('sunspots.csv', <http.client.HTTPMessage at 0x7f5864f73b10>)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["with open('sunspots.csv') as csvfile:\n","    reader = csv.reader(csvfile, delimiter=',')\n","    next(reader)\n","    i = 0\n","    for row in reader:\n","        print(row)\n","        i+=1\n","        if i > 10:\n","            break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ERrNxHQqnb07","executionInfo":{"status":"ok","timestamp":1643990428334,"user_tz":-540,"elapsed":6,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}},"outputId":"e89a2566-b478-4630-c17d-37e11f68ae52"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['0', '1749-01-31', '96.7']\n","['1', '1749-02-28', '104.3']\n","['2', '1749-03-31', '116.7']\n","['3', '1749-04-30', '92.8']\n","['4', '1749-05-31', '141.7']\n","['5', '1749-06-30', '139.2']\n","['6', '1749-07-31', '158.0']\n","['7', '1749-08-31', '110.5']\n","['8', '1749-09-30', '126.5']\n","['9', '1749-10-31', '125.8']\n","['10', '1749-11-30', '264.3']\n"]}]},{"cell_type":"code","source":["sunspots = []\n","time_step = []"],"metadata":{"id":"6j2HQ1mQnb3y","executionInfo":{"status":"ok","timestamp":1643990428334,"user_tz":-540,"elapsed":3,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["with open('sunspots.csv') as csvfile:\n","    reader = csv.reader(csvfile, delimiter=',')\n","    # 첫 줄은 header이므로 skip 합니다.\n","    next(reader)\n","    for row in reader:\n","        sunspots.append(float(row[2]))\n","        time_step.append(int(row[0]))"],"metadata":{"id":"u7utrW1Unb6K","executionInfo":{"status":"ok","timestamp":1643990428911,"user_tz":-540,"elapsed":7,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["series = np.array(sunspots)\n","time = np.array(time_step)"],"metadata":{"id":"OBxdteC2nb8_","executionInfo":{"status":"ok","timestamp":1643990428911,"user_tz":-540,"elapsed":6,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["split_time = 3000\n","x_train = series[:split_time]\n","x_valid = series[split_time:]"],"metadata":{"id":"1akJdkYxnb_o","executionInfo":{"status":"ok","timestamp":1643990428914,"user_tz":-540,"elapsed":9,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# 윈도우 사이즈\n","window_size=30\n","# 배치 사이즈\n","batch_size = 32\n","# 셔플 사이즈\n","shuffle_size = 1000"],"metadata":{"id":"o8kZQwysnuy-","executionInfo":{"status":"ok","timestamp":1643990428915,"user_tz":-540,"elapsed":10,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n","    series = tf.expand_dims(series, axis=-1)\n","    ds = tf.data.Dataset.from_tensor_slices(series)\n","    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n","    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n","    ds = ds.shuffle(shuffle_buffer)\n","    ds = ds.map(lambda w: (w[:-1], w[1:]))\n","    return ds.batch(batch_size).prefetch(1)"],"metadata":{"id":"Y3Iwspawnuvv","executionInfo":{"status":"ok","timestamp":1643990428915,"user_tz":-540,"elapsed":9,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["train_set = windowed_dataset(x_train, \n","                             window_size=window_size, \n","                             batch_size=batch_size,\n","                             shuffle_buffer=shuffle_size)\n","\n","validation_set = windowed_dataset(x_valid, \n","                                  window_size=window_size,\n","                                  batch_size=batch_size,\n","                                  shuffle_buffer=shuffle_size)"],"metadata":{"id":"QrMwJ3t5ncCf","executionInfo":{"status":"ok","timestamp":1643990431876,"user_tz":-540,"elapsed":2969,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["model = Sequential([\n","    tf.keras.layers.Conv1D(60, kernel_size=5,\n","                         padding=\"causal\",\n","                         activation=\"relu\",\n","                         input_shape=[None, 1]),\n","    tf.keras.layers.LSTM(60, return_sequences=True),\n","    tf.keras.layers.LSTM(60, return_sequences=True),\n","    tf.keras.layers.Dense(30, activation=\"relu\"),\n","    tf.keras.layers.Dense(10, activation=\"relu\"),\n","    tf.keras.layers.Dense(1),\n","    tf.keras.layers.Lambda(lambda x: x * 400)\n","])\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YcRl12sIncE0","executionInfo":{"status":"ok","timestamp":1643990432870,"user_tz":-540,"elapsed":997,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}},"outputId":"82b6d3e6-c8a4-4666-d0cf-89d467b0d1be"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, None, 60)          360       \n","                                                                 \n"," lstm (LSTM)                 (None, None, 60)          29040     \n","                                                                 \n"," lstm_1 (LSTM)               (None, None, 60)          29040     \n","                                                                 \n"," dense (Dense)               (None, None, 30)          1830      \n","                                                                 \n"," dense_1 (Dense)             (None, None, 10)          310       \n","                                                                 \n"," dense_2 (Dense)             (None, None, 1)           11        \n","                                                                 \n"," lambda (Lambda)             (None, None, 1)           0         \n","                                                                 \n","=================================================================\n","Total params: 60,591\n","Trainable params: 60,591\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["optimizer = SGD(lr=1e-5, momentum=0.9)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8tvVdFLPncH9","executionInfo":{"status":"ok","timestamp":1643990432870,"user_tz":-540,"elapsed":6,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}},"outputId":"50a82112-3022-4b48-c0b5-92a556f46cdd"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]}]},{"cell_type":"code","source":["loss= Huber()"],"metadata":{"id":"M2iFFnq9ncKA","executionInfo":{"status":"ok","timestamp":1643990432871,"user_tz":-540,"elapsed":4,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["model.compile(loss=loss,\n","              optimizer=optimizer,\n","              metrics=[\"mae\"])"],"metadata":{"id":"b6asH5Nin7Vh","executionInfo":{"status":"ok","timestamp":1643990432871,"user_tz":-540,"elapsed":4,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["checkpoint_path = 'tmp_checkpoint.ckpt'\n","checkpoint = ModelCheckpoint(checkpoint_path, \n","                             save_weights_only=True, \n","                             save_best_only=True, \n","                             monitor='val_mae',\n","                             verbose=1)"],"metadata":{"id":"YE8BYDPLn7X8","executionInfo":{"status":"ok","timestamp":1643990432873,"user_tz":-540,"elapsed":5,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["epochs=100"],"metadata":{"id":"55Z7v1h0n7ah","executionInfo":{"status":"ok","timestamp":1643990432873,"user_tz":-540,"elapsed":5,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["history = model.fit(train_set, \n","                    validation_data=(validation_set), \n","                    epochs=epochs, \n","                    callbacks=[checkpoint],\n","                   )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PxW_Em9Dn7cq","executionInfo":{"status":"ok","timestamp":1643990882057,"user_tz":-540,"elapsed":449189,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}},"outputId":"5acdb82f-4d32-48d0-ee7b-f554657171e6"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","     93/Unknown - 16s 25ms/step - loss: 29.7423 - mae: 30.2372\n","Epoch 00001: val_mae improved from inf to 21.61208, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 18s 44ms/step - loss: 29.7423 - mae: 30.2372 - val_loss: 21.1168 - val_mae: 21.6121\n","Epoch 2/100\n","93/93 [==============================] - ETA: 0s - loss: 20.8954 - mae: 21.3886\n","Epoch 00002: val_mae improved from 21.61208 to 17.41523, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 3s 25ms/step - loss: 20.8954 - mae: 21.3886 - val_loss: 16.9214 - val_mae: 17.4152\n","Epoch 3/100\n","91/93 [============================>.] - ETA: 0s - loss: 19.9276 - mae: 20.4208\n","Epoch 00003: val_mae improved from 17.41523 to 15.50385, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 3s 26ms/step - loss: 19.8820 - mae: 20.3751 - val_loss: 15.0124 - val_mae: 15.5038\n","Epoch 4/100\n","91/93 [============================>.] - ETA: 0s - loss: 19.1260 - mae: 19.6186\n","Epoch 00004: val_mae improved from 15.50385 to 15.17237, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 3s 26ms/step - loss: 19.1620 - mae: 19.6546 - val_loss: 14.6799 - val_mae: 15.1724\n","Epoch 5/100\n","93/93 [==============================] - ETA: 0s - loss: 18.2556 - mae: 18.7477\n","Epoch 00005: val_mae improved from 15.17237 to 15.10024, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 3s 27ms/step - loss: 18.2556 - mae: 18.7477 - val_loss: 14.6087 - val_mae: 15.1002\n","Epoch 6/100\n","91/93 [============================>.] - ETA: 0s - loss: 18.0851 - mae: 18.5769\n","Epoch 00006: val_mae did not improve from 15.10024\n","93/93 [==============================] - 3s 26ms/step - loss: 18.0699 - mae: 18.5617 - val_loss: 14.9696 - val_mae: 15.4604\n","Epoch 7/100\n","92/93 [============================>.] - ETA: 0s - loss: 18.2233 - mae: 18.7155\n","Epoch 00007: val_mae improved from 15.10024 to 14.61420, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 3s 25ms/step - loss: 18.2300 - mae: 18.7222 - val_loss: 14.1237 - val_mae: 14.6142\n","Epoch 8/100\n","93/93 [==============================] - ETA: 0s - loss: 17.9618 - mae: 18.4536\n","Epoch 00008: val_mae did not improve from 14.61420\n","93/93 [==============================] - 3s 25ms/step - loss: 17.9618 - mae: 18.4536 - val_loss: 14.7408 - val_mae: 15.2311\n","Epoch 9/100\n","93/93 [==============================] - ETA: 0s - loss: 17.7494 - mae: 18.2405\n","Epoch 00009: val_mae did not improve from 14.61420\n","93/93 [==============================] - 3s 26ms/step - loss: 17.7494 - mae: 18.2405 - val_loss: 16.2666 - val_mae: 16.7605\n","Epoch 10/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.5266 - mae: 18.0180\n","Epoch 00010: val_mae improved from 14.61420 to 14.40847, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 3s 26ms/step - loss: 17.5518 - mae: 18.0432 - val_loss: 13.9188 - val_mae: 14.4085\n","Epoch 11/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.5199 - mae: 18.0109\n","Epoch 00011: val_mae improved from 14.40847 to 14.33413, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 3s 26ms/step - loss: 17.5217 - mae: 18.0127 - val_loss: 13.8446 - val_mae: 14.3341\n","Epoch 12/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.4166 - mae: 17.9077\n","Epoch 00012: val_mae improved from 14.33413 to 14.27554, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 3s 26ms/step - loss: 17.4121 - mae: 17.9032 - val_loss: 13.7860 - val_mae: 14.2755\n","Epoch 13/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.3345 - mae: 17.8254\n","Epoch 00013: val_mae did not improve from 14.27554\n","93/93 [==============================] - 3s 26ms/step - loss: 17.3562 - mae: 17.8471 - val_loss: 14.1131 - val_mae: 14.6044\n","Epoch 14/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.2652 - mae: 17.7559\n","Epoch 00014: val_mae did not improve from 14.27554\n","93/93 [==============================] - 3s 26ms/step - loss: 17.2627 - mae: 17.7534 - val_loss: 13.9510 - val_mae: 14.4392\n","Epoch 15/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.4784 - mae: 17.9694\n","Epoch 00015: val_mae did not improve from 14.27554\n","93/93 [==============================] - 3s 26ms/step - loss: 17.4609 - mae: 17.9520 - val_loss: 14.0141 - val_mae: 14.5053\n","Epoch 16/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.1754 - mae: 17.6661\n","Epoch 00016: val_mae improved from 14.27554 to 14.22100, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 3s 26ms/step - loss: 17.1803 - mae: 17.6711 - val_loss: 13.7310 - val_mae: 14.2210\n","Epoch 17/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.0538 - mae: 17.5444\n","Epoch 00017: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 17.0795 - mae: 17.5701 - val_loss: 14.2507 - val_mae: 14.7403\n","Epoch 18/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.1215 - mae: 17.6124\n","Epoch 00018: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 17.1347 - mae: 17.6255 - val_loss: 13.8520 - val_mae: 14.3419\n","Epoch 19/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.1446 - mae: 17.6352\n","Epoch 00019: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 17.1468 - mae: 17.6374 - val_loss: 13.9394 - val_mae: 14.4284\n","Epoch 20/100\n","93/93 [==============================] - ETA: 0s - loss: 17.0485 - mae: 17.5389\n","Epoch 00020: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 17.0485 - mae: 17.5389 - val_loss: 14.3860 - val_mae: 14.8744\n","Epoch 21/100\n","93/93 [==============================] - ETA: 0s - loss: 17.0739 - mae: 17.5647\n","Epoch 00021: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 17.0739 - mae: 17.5647 - val_loss: 13.7571 - val_mae: 14.2461\n","Epoch 22/100\n","91/93 [============================>.] - ETA: 0s - loss: 17.1836 - mae: 17.6743\n","Epoch 00022: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 17.1564 - mae: 17.6472 - val_loss: 14.0140 - val_mae: 14.5048\n","Epoch 23/100\n","92/93 [============================>.] - ETA: 0s - loss: 17.0507 - mae: 17.5412\n","Epoch 00023: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 25ms/step - loss: 17.0398 - mae: 17.5303 - val_loss: 13.9046 - val_mae: 14.3938\n","Epoch 24/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.9933 - mae: 17.4840\n","Epoch 00024: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.9910 - mae: 17.4818 - val_loss: 14.0028 - val_mae: 14.4926\n","Epoch 25/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.9022 - mae: 17.3928\n","Epoch 00025: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.8981 - mae: 17.3887 - val_loss: 14.0622 - val_mae: 14.5525\n","Epoch 26/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.8521 - mae: 17.3425\n","Epoch 00026: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.8511 - mae: 17.3415 - val_loss: 13.9387 - val_mae: 14.4298\n","Epoch 27/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.7521 - mae: 17.2426\n","Epoch 00027: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.7540 - mae: 17.2445 - val_loss: 14.1222 - val_mae: 14.6123\n","Epoch 28/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.8747 - mae: 17.3651\n","Epoch 00028: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.8757 - mae: 17.3661 - val_loss: 14.1204 - val_mae: 14.6081\n","Epoch 29/100\n","93/93 [==============================] - ETA: 0s - loss: 16.8957 - mae: 17.3860\n","Epoch 00029: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.8957 - mae: 17.3860 - val_loss: 14.2714 - val_mae: 14.7615\n","Epoch 30/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.8065 - mae: 17.2970\n","Epoch 00030: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.8064 - mae: 17.2968 - val_loss: 14.0069 - val_mae: 14.4980\n","Epoch 31/100\n","93/93 [==============================] - ETA: 0s - loss: 16.6517 - mae: 17.1418\n","Epoch 00031: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.6517 - mae: 17.1418 - val_loss: 14.3233 - val_mae: 14.8128\n","Epoch 32/100\n","93/93 [==============================] - ETA: 0s - loss: 16.7019 - mae: 17.1918\n","Epoch 00032: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 27ms/step - loss: 16.7019 - mae: 17.1918 - val_loss: 14.1397 - val_mae: 14.6301\n","Epoch 33/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.6850 - mae: 17.1753\n","Epoch 00033: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.6908 - mae: 17.1811 - val_loss: 15.2205 - val_mae: 15.7105\n","Epoch 34/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.6891 - mae: 17.1790\n","Epoch 00034: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.6752 - mae: 17.1651 - val_loss: 13.8619 - val_mae: 14.3514\n","Epoch 35/100\n","93/93 [==============================] - ETA: 0s - loss: 16.5998 - mae: 17.0899\n","Epoch 00035: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.5998 - mae: 17.0899 - val_loss: 14.9168 - val_mae: 15.4064\n","Epoch 36/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.7119 - mae: 17.2023\n","Epoch 00036: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 27ms/step - loss: 16.7272 - mae: 17.2176 - val_loss: 14.2787 - val_mae: 14.7694\n","Epoch 37/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.6058 - mae: 17.0960\n","Epoch 00037: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.5885 - mae: 17.0788 - val_loss: 14.0438 - val_mae: 14.5319\n","Epoch 38/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.5757 - mae: 17.0655\n","Epoch 00038: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.5555 - mae: 17.0453 - val_loss: 14.3182 - val_mae: 14.8080\n","Epoch 39/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.6129 - mae: 17.1029\n","Epoch 00039: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 27ms/step - loss: 16.6248 - mae: 17.1149 - val_loss: 14.5550 - val_mae: 15.0436\n","Epoch 40/100\n","93/93 [==============================] - ETA: 0s - loss: 16.4359 - mae: 16.9258\n","Epoch 00040: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 27ms/step - loss: 16.4359 - mae: 16.9258 - val_loss: 14.4305 - val_mae: 14.9209\n","Epoch 41/100\n","93/93 [==============================] - ETA: 0s - loss: 16.5499 - mae: 17.0399\n","Epoch 00041: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 27ms/step - loss: 16.5499 - mae: 17.0399 - val_loss: 14.0086 - val_mae: 14.4989\n","Epoch 42/100\n","93/93 [==============================] - ETA: 0s - loss: 16.5185 - mae: 17.0088\n","Epoch 00042: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 27ms/step - loss: 16.5185 - mae: 17.0088 - val_loss: 13.8063 - val_mae: 14.2940\n","Epoch 43/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.5335 - mae: 17.0236\n","Epoch 00043: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.5599 - mae: 17.0500 - val_loss: 13.8009 - val_mae: 14.2875\n","Epoch 44/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.4056 - mae: 16.8957\n","Epoch 00044: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.4076 - mae: 16.8977 - val_loss: 13.9004 - val_mae: 14.3901\n","Epoch 45/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.4495 - mae: 16.9393\n","Epoch 00045: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.4289 - mae: 16.9188 - val_loss: 14.0524 - val_mae: 14.5393\n","Epoch 46/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.4551 - mae: 16.9446\n","Epoch 00046: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 25ms/step - loss: 16.4589 - mae: 16.9484 - val_loss: 14.5887 - val_mae: 15.0791\n","Epoch 47/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.5096 - mae: 16.9999\n","Epoch 00047: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.5125 - mae: 17.0028 - val_loss: 13.8739 - val_mae: 14.3623\n","Epoch 48/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.3513 - mae: 16.8411\n","Epoch 00048: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.3606 - mae: 16.8504 - val_loss: 14.2263 - val_mae: 14.7154\n","Epoch 49/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.3564 - mae: 16.8459\n","Epoch 00049: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.3539 - mae: 16.8435 - val_loss: 13.9923 - val_mae: 14.4820\n","Epoch 50/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.3815 - mae: 16.8714\n","Epoch 00050: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.3834 - mae: 16.8733 - val_loss: 14.1823 - val_mae: 14.6720\n","Epoch 51/100\n","93/93 [==============================] - ETA: 0s - loss: 16.2850 - mae: 16.7748\n","Epoch 00051: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.2850 - mae: 16.7748 - val_loss: 13.9232 - val_mae: 14.4124\n","Epoch 52/100\n","93/93 [==============================] - ETA: 0s - loss: 16.3151 - mae: 16.8048\n","Epoch 00052: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.3151 - mae: 16.8048 - val_loss: 14.0488 - val_mae: 14.5382\n","Epoch 53/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.3907 - mae: 16.8805\n","Epoch 00053: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.3943 - mae: 16.8841 - val_loss: 14.0535 - val_mae: 14.5427\n","Epoch 54/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.2379 - mae: 16.7277\n","Epoch 00054: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 26ms/step - loss: 16.2359 - mae: 16.7257 - val_loss: 14.3738 - val_mae: 14.8624\n","Epoch 55/100\n","93/93 [==============================] - ETA: 0s - loss: 16.2717 - mae: 16.7612\n","Epoch 00055: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 27ms/step - loss: 16.2717 - mae: 16.7612 - val_loss: 14.1851 - val_mae: 14.6758\n","Epoch 56/100\n","93/93 [==============================] - ETA: 0s - loss: 16.2585 - mae: 16.7482\n","Epoch 00056: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 27ms/step - loss: 16.2585 - mae: 16.7482 - val_loss: 14.2212 - val_mae: 14.7102\n","Epoch 57/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.2266 - mae: 16.7165\n","Epoch 00057: val_mae did not improve from 14.22100\n","93/93 [==============================] - 3s 27ms/step - loss: 16.2403 - mae: 16.7302 - val_loss: 14.5185 - val_mae: 15.0066\n","Epoch 58/100\n","93/93 [==============================] - ETA: 0s - loss: 16.3398 - mae: 16.8296\n","Epoch 00058: val_mae improved from 14.22100 to 14.21673, saving model to tmp_checkpoint.ckpt\n","93/93 [==============================] - 3s 26ms/step - loss: 16.3398 - mae: 16.8296 - val_loss: 13.7263 - val_mae: 14.2167\n","Epoch 59/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.2242 - mae: 16.7137\n","Epoch 00059: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 26ms/step - loss: 16.2346 - mae: 16.7241 - val_loss: 14.3647 - val_mae: 14.8529\n","Epoch 60/100\n","93/93 [==============================] - ETA: 0s - loss: 16.3593 - mae: 16.8489\n","Epoch 00060: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.3593 - mae: 16.8489 - val_loss: 14.3759 - val_mae: 14.8640\n","Epoch 61/100\n","93/93 [==============================] - ETA: 0s - loss: 16.3497 - mae: 16.8397\n","Epoch 00061: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.3497 - mae: 16.8397 - val_loss: 14.7748 - val_mae: 15.2622\n","Epoch 62/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.3862 - mae: 16.8759\n","Epoch 00062: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 26ms/step - loss: 16.3864 - mae: 16.8761 - val_loss: 14.2231 - val_mae: 14.7107\n","Epoch 63/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.2143 - mae: 16.7039\n","Epoch 00063: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 26ms/step - loss: 16.2454 - mae: 16.7350 - val_loss: 14.2638 - val_mae: 14.7539\n","Epoch 64/100\n","93/93 [==============================] - ETA: 0s - loss: 16.2613 - mae: 16.7514\n","Epoch 00064: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 26ms/step - loss: 16.2613 - mae: 16.7514 - val_loss: 14.5369 - val_mae: 15.0270\n","Epoch 65/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.1605 - mae: 16.6502\n","Epoch 00065: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.1775 - mae: 16.6672 - val_loss: 15.0591 - val_mae: 15.5467\n","Epoch 66/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.2453 - mae: 16.7349\n","Epoch 00066: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.2435 - mae: 16.7331 - val_loss: 14.0664 - val_mae: 14.5556\n","Epoch 67/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.1637 - mae: 16.6533\n","Epoch 00067: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.1615 - mae: 16.6511 - val_loss: 14.4881 - val_mae: 14.9751\n","Epoch 68/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.2085 - mae: 16.6982\n","Epoch 00068: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.2170 - mae: 16.7066 - val_loss: 13.8342 - val_mae: 14.3209\n","Epoch 69/100\n","93/93 [==============================] - ETA: 0s - loss: 16.1636 - mae: 16.6532\n","Epoch 00069: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.1636 - mae: 16.6532 - val_loss: 14.3532 - val_mae: 14.8409\n","Epoch 70/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.0887 - mae: 16.5779\n","Epoch 00070: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.1078 - mae: 16.5971 - val_loss: 14.8035 - val_mae: 15.2938\n","Epoch 71/100\n","93/93 [==============================] - ETA: 0s - loss: 16.1530 - mae: 16.6424\n","Epoch 00071: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.1530 - mae: 16.6424 - val_loss: 15.4500 - val_mae: 15.9401\n","Epoch 72/100\n","93/93 [==============================] - ETA: 0s - loss: 16.5011 - mae: 16.9914\n","Epoch 00072: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.5011 - mae: 16.9914 - val_loss: 14.0895 - val_mae: 14.5781\n","Epoch 73/100\n","93/93 [==============================] - ETA: 0s - loss: 16.0830 - mae: 16.5726\n","Epoch 00073: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.0830 - mae: 16.5726 - val_loss: 14.0169 - val_mae: 14.5089\n","Epoch 74/100\n","93/93 [==============================] - ETA: 0s - loss: 16.0372 - mae: 16.5267\n","Epoch 00074: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 26ms/step - loss: 16.0372 - mae: 16.5267 - val_loss: 14.2112 - val_mae: 14.7011\n","Epoch 75/100\n","93/93 [==============================] - ETA: 0s - loss: 16.0213 - mae: 16.5107\n","Epoch 00075: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.0213 - mae: 16.5107 - val_loss: 14.0097 - val_mae: 14.4990\n","Epoch 76/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.0418 - mae: 16.5310\n","Epoch 00076: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 26ms/step - loss: 16.0549 - mae: 16.5442 - val_loss: 14.4380 - val_mae: 14.9269\n","Epoch 77/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.0456 - mae: 16.5350\n","Epoch 00077: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.0366 - mae: 16.5260 - val_loss: 14.3801 - val_mae: 14.8709\n","Epoch 78/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.0315 - mae: 16.5210\n","Epoch 00078: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.0598 - mae: 16.5492 - val_loss: 14.0916 - val_mae: 14.5812\n","Epoch 79/100\n","93/93 [==============================] - ETA: 0s - loss: 16.0949 - mae: 16.5843\n","Epoch 00079: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.0949 - mae: 16.5843 - val_loss: 14.4709 - val_mae: 14.9607\n","Epoch 80/100\n","92/93 [============================>.] - ETA: 0s - loss: 15.9578 - mae: 16.4469\n","Epoch 00080: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 15.9778 - mae: 16.4670 - val_loss: 14.6638 - val_mae: 15.1525\n","Epoch 81/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.1909 - mae: 16.6806\n","Epoch 00081: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.1539 - mae: 16.6436 - val_loss: 14.8140 - val_mae: 15.3016\n","Epoch 82/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.0274 - mae: 16.5168\n","Epoch 00082: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.0419 - mae: 16.5312 - val_loss: 14.4417 - val_mae: 14.9328\n","Epoch 83/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.1246 - mae: 16.6142\n","Epoch 00083: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.1248 - mae: 16.6144 - val_loss: 14.5658 - val_mae: 15.0556\n","Epoch 84/100\n","92/93 [============================>.] - ETA: 0s - loss: 15.9316 - mae: 16.4208\n","Epoch 00084: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 15.9309 - mae: 16.4201 - val_loss: 14.1620 - val_mae: 14.6503\n","Epoch 85/100\n","91/93 [============================>.] - ETA: 0s - loss: 16.0148 - mae: 16.5042\n","Epoch 00085: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 15.9897 - mae: 16.4791 - val_loss: 14.3916 - val_mae: 14.8812\n","Epoch 86/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.0105 - mae: 16.5000\n","Epoch 00086: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.0015 - mae: 16.4911 - val_loss: 14.6844 - val_mae: 15.1747\n","Epoch 87/100\n","92/93 [============================>.] - ETA: 0s - loss: 15.9818 - mae: 16.4712\n","Epoch 00087: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 15.9935 - mae: 16.4829 - val_loss: 14.3717 - val_mae: 14.8600\n","Epoch 88/100\n","91/93 [============================>.] - ETA: 0s - loss: 15.9571 - mae: 16.4466\n","Epoch 00088: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 28ms/step - loss: 15.9736 - mae: 16.4631 - val_loss: 14.9422 - val_mae: 15.4296\n","Epoch 89/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.0754 - mae: 16.5651\n","Epoch 00089: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 28ms/step - loss: 16.0605 - mae: 16.5501 - val_loss: 14.5618 - val_mae: 15.0529\n","Epoch 90/100\n","92/93 [============================>.] - ETA: 0s - loss: 16.2046 - mae: 16.6944\n","Epoch 00090: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.1980 - mae: 16.6877 - val_loss: 14.3439 - val_mae: 14.8354\n","Epoch 91/100\n","93/93 [==============================] - ETA: 0s - loss: 15.9241 - mae: 16.4139\n","Epoch 00091: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 15.9241 - mae: 16.4139 - val_loss: 14.2482 - val_mae: 14.7365\n","Epoch 92/100\n","93/93 [==============================] - ETA: 0s - loss: 15.9634 - mae: 16.4529\n","Epoch 00092: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 15.9634 - mae: 16.4529 - val_loss: 14.3340 - val_mae: 14.8231\n","Epoch 93/100\n","93/93 [==============================] - ETA: 0s - loss: 15.8844 - mae: 16.3740\n","Epoch 00093: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 15.8844 - mae: 16.3740 - val_loss: 14.1702 - val_mae: 14.6596\n","Epoch 94/100\n","91/93 [============================>.] - ETA: 0s - loss: 15.8848 - mae: 16.3741\n","Epoch 00094: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 15.8817 - mae: 16.3709 - val_loss: 14.5985 - val_mae: 15.0886\n","Epoch 95/100\n","91/93 [============================>.] - ETA: 0s - loss: 15.9141 - mae: 16.4036\n","Epoch 00095: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 15.9328 - mae: 16.4223 - val_loss: 14.2953 - val_mae: 14.7841\n","Epoch 96/100\n","91/93 [============================>.] - ETA: 0s - loss: 15.9770 - mae: 16.4664\n","Epoch 00096: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 15.9508 - mae: 16.4403 - val_loss: 14.5198 - val_mae: 15.0098\n","Epoch 97/100\n","93/93 [==============================] - ETA: 0s - loss: 16.0636 - mae: 16.5535\n","Epoch 00097: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 16.0636 - mae: 16.5535 - val_loss: 14.0886 - val_mae: 14.5779\n","Epoch 98/100\n","92/93 [============================>.] - ETA: 0s - loss: 15.9629 - mae: 16.4520\n","Epoch 00098: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 26ms/step - loss: 15.9476 - mae: 16.4366 - val_loss: 14.9125 - val_mae: 15.4016\n","Epoch 99/100\n","91/93 [============================>.] - ETA: 0s - loss: 15.8533 - mae: 16.3425\n","Epoch 00099: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 15.8895 - mae: 16.3788 - val_loss: 15.1724 - val_mae: 15.6615\n","Epoch 100/100\n","92/93 [============================>.] - ETA: 0s - loss: 15.7410 - mae: 16.2307\n","Epoch 00100: val_mae did not improve from 14.21673\n","93/93 [==============================] - 3s 27ms/step - loss: 15.7518 - mae: 16.2415 - val_loss: 14.4919 - val_mae: 14.9800\n"]}]},{"cell_type":"code","source":["model.load_weights(checkpoint_path)"],"metadata":{"id":"imIfqd5eoDN3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643990882057,"user_tz":-540,"elapsed":12,"user":{"displayName":"JaeYoung Hwang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08071223562055378805"}},"outputId":"24b8b0ed-36fc-466c-843b-3211d4a297dc"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f5768146dd0>"]},"metadata":{},"execution_count":20}]}]}